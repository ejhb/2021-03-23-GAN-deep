{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gdown\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"celeba_gan\")\n",
    "\n",
    "# url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
    "# output = \"celeba_gan/data.zip\"\n",
    "# gdown.download(url, output, quiet=True)\n",
    "\n",
    "# with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
    "#     zipobj.extractall(\"celeba_gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\"../data\", \n",
    "                                                            label_mode=None, \n",
    "                                                            image_size=(64, 64), \n",
    "                                                            batch_size=32)\n",
    "dataset = dataset.map(lambda x: x / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-03-23T16:16:29.841202</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 231.84 \n",
       "L 231.84 231.84 \n",
       "L 231.84 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p412012cf24)\">\n",
       "    <image height=\"218\" id=\"image6729af9f5b\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p412012cf24\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DT (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DT (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DT (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  8/313 [..............................] - ETA: 31:30 - d_loss: 0.6556 - g_loss: 0.5719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ee357eed991d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m gan.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGANMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
